{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "664fecb2-377b-46e6-9bd8-0108cf3b896f",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Twitter Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be972e4b-d490-4fae-b014-ac9466b2fa36",
   "metadata": {},
   "source": [
    "## Motivation/State of the Art\n",
    "### Motivation\n",
    "Die Analyse von Stimmungen in sozialen Medien hat in den letzten Jahren an Bedeutung gewonnen. Unternehmen, Forscher und Organisationen verwenden Sentimentanalyse, um Einblicke in die Meinungen der Öffentlichkeit zu gewinnen. Dieses Projekt konzentriert sich auf die Sentimentanalyse von Twitter-Tweets und nutzt Methoden des Natural Language Processing (NLP) um Tweets als positiv oder negativ zu klassifizieren.\n",
    "\n",
    "Das Ziel ist es, verschiedene Modelle und Architekturen zu vergleichen, die in der Sentimentanalyse verwendet werden, um die beste Leistung hinsichtlich Genauigkeit und Robustheit zu ermitteln. Das Projekt ist inspiriert von bestehenden Arbeiten in der Stimmungsanalyse, die Techniken wie LSTM, Vorverarbeitung von Textdaten und Explainable AI (z. B. LIME) einsetzen.\n",
    "\n",
    "### State of the Art\n",
    "\n",
    "Transformer-basierte Modelle (z. B. BERT, RoBERTa): Transformer-Modelle dominieren die NLP-Landschaft mit herausragender Leistung bei Textklassifikationsaufgaben.\n",
    "\n",
    "Rekurrente neuronale Netze (RNNs) und LSTM: Diese Modelle können den sequentiellen Kontext von Texten erfassen und sind besonders nützlich für Tweets, die oft aufeinander aufbauen.\n",
    "\n",
    "Explainable AI (LIME): LIME erklärt die Vorhersagen von Modellen durch die Analyse der Bedeutung einzelner Textmerkmale (z. B. Wörter oder Phrasen)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5938565e-ff18-4d00-926d-a81ef58190a1",
   "metadata": {},
   "source": [
    "# Importieren von Bibliotheken\n",
    "Importieren wir die benötigten Bibliotheken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c10d904-83f9-4452-bba5-f8137eeb08f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import rcParams\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import string\n",
    "import tensorflow\n",
    "from sklearn.metrics  import classification_report ,confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa42d54-7866-4dfb-b125-002d4d23e4ac",
   "metadata": {},
   "source": [
    "# Daten einlesen und vorbereiten\n",
    "Wir verwenden ein vorhandenes Twitter-Datenset, das Tweets und ihre Labels (0 = negativ, 4 = positiv) enthält:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8ca1fce-dd30-48b6-9d6e-85e9bcd22615",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", engine=\"python\")\n",
    "data.columns = [\"label\", \"id\", \"date\", \"query\", \"username\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30e35526-f0c0-421e-908d-342a937c7b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label          id                          date     query       username  \\\n",
       "0      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
       "1      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
       "2      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
       "3      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
       "4      0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
       "\n",
       "                                                text  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb020308-60a6-40d7-8d12-17d4a58a7423",
   "metadata": {},
   "source": [
    "die Spalten auf \"text\" und \"label\" reduzieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad0a007c-0550-45f4-a58b-e36c0a1c6067",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[['text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81bde87c-c2c6-4774-8313-581dcce5a80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  is upset that he can't update his Facebook by ...      0\n",
       "1  @Kenichan I dived many times for the ball. Man...      0\n",
       "2    my whole body feels itchy and like its on fire       0\n",
       "3  @nationwideclass no, it's not behaving at all....      0\n",
       "4                      @Kwesidei not the whole crew       0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "124e4c3d-dc65-4287-b153-52df66a22586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3fcf99b-b0a8-4a17-b205-1da881f6ebbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length: 1599999\n"
     ]
    }
   ],
   "source": [
    "print('data length:', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cec1941c-65f4-4557-8ba8-079a7fcaf531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58617553-5304-406e-a193-412556b4f9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                       text  label\n",
       "0        is upset that he can't update his Facebook by ...      0\n",
       "1        @Kenichan I dived many times for the ball. Man...      0\n",
       "2          my whole body feels itchy and like its on fire       0\n",
       "3        @nationwideclass no, it's not behaving at all....      0\n",
       "4                            @Kwesidei not the whole crew       0\n",
       "...                                                    ...    ...\n",
       "1599994  Just woke up. Having no school is the best fee...      4\n",
       "1599995  TheWDB.com - Very cool to hear old Walt interv...      4\n",
       "1599996  Are you ready for your MoJo Makeover? Ask me f...      4\n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! ...      4\n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity...      4\n",
       "\n",
       "[1599999 rows x 2 columns]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd6c53ab-53f9-472b-adfd-f382902f93c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     object\n",
       "label     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b83e46-42ef-48fa-a73c-039855546979",
   "metadata": {},
   "source": [
    "Das Label der Datensätze der Positivität von \"4\" auf \"1\" ändern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f43d7a6e-908c-488d-abdf-6964133a9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['label']==4, 'label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "173bbc6f-6890-49d6-b5bf-b1c2ae82c771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                       text  label\n",
       "0        is upset that he can't update his Facebook by ...      0\n",
       "1        @Kenichan I dived many times for the ball. Man...      0\n",
       "2          my whole body feels itchy and like its on fire       0\n",
       "3        @nationwideclass no, it's not behaving at all....      0\n",
       "4                            @Kwesidei not the whole crew       0\n",
       "...                                                    ...    ...\n",
       "1599994  Just woke up. Having no school is the best fee...      1\n",
       "1599995  TheWDB.com - Very cool to hear old Walt interv...      1\n",
       "1599996  Are you ready for your MoJo Makeover? Ask me f...      1\n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! ...      1\n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity...      1\n",
       "\n",
       "[1599999 rows x 2 columns]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b625f3dd-9f79-48ac-bda1-81e5272525df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    800000\n",
       "0    799999\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ea0141a-9939-4333-a512-d24b17ee5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data[data['label'] == 1]\n",
    "data_neg = data[data['label'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f307dcb-84e9-4c7e-ba6c-62dbebbe4522",
   "metadata": {},
   "source": [
    "Datensatz reduzieren da er für unsere Hardware zu anspruchsvoll ist.\n",
    "\n",
    "Das ursprüngliche Datenset ist sehr groß (1,6 Millionen Zeilen), was den Entwicklungsprozess verlangsamen würde. Eine kleinere Stichprobe reicht aus, um den Workflow zu testen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b204e3f4-484a-4d51-9a42-a7ed6936b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data_pos.iloc[:int(100000)]\n",
    "data_neg = data_neg.iloc[:int(100000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32e96fa1-9152-4acf-8f5b-dd047c09fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_pos, data_neg])\n",
    "data[\"label\"].value_counts()\n",
    "del data_pos\n",
    "del data_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f9d09-f8d7-41ad-89df-17fc7b909d2d",
   "metadata": {},
   "source": [
    "# Data Preprocessing (Text Cleaning)\n",
    "Tweets enthalten oft Rauschen wie URLs, Benutzernamen, Sonderzeichen und Abkürzungen. Diese Informationen stören das Modelltraining und müssen entfernt werden.\n",
    "\n",
    "Wie:\n",
    "1. Abkürzungen ausschreiben\n",
    "2. alles in Lowercase umwandeln\n",
    "3. Stopwords aus den Texten entfernen\n",
    "4. URLs, Benutzernamen und Sonderzeichen aus den Texten entfernen\n",
    "5. Lemmatization\n",
    "6. Daten tokenisieren & in Sequenzen umwandeln\n",
    "7. In Trainings/Validierungs & Test-Datenset aufteilen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60fe4fc9-8564-43ba-b316-42172c20d55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                      text  label\n",
       "799999       I LOVE @Health4UandPets u guys r the best!!       1\n",
       "800000  im meeting up with one of my besties tonight! ...      1\n",
       "800001  @DaRealSunisaKim Thanks for the Twitter add, S...      1\n",
       "800002  Being sick can be really cheap when it hurts t...      1\n",
       "800003    @LovesBrooklyn2 he has that effect on everyone       1\n",
       "...                                                   ...    ...\n",
       "99995   looks like my routers broke  more tweets from ...      0\n",
       "99996   i really dont want to be in college right now....      0\n",
       "99997                         @flossa  *offers you pepto*      0\n",
       "99998   @JosieHobo I WOULD SOOOOO BE THERE IF I DIDN'T...      0\n",
       "99999   OMG, I just moisturised and my legs are BURNIN...      0\n",
       "\n",
       "[200000 rows x 2 columns]>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1368a94-c77b-4f5a-b663-9ea3c2206769",
   "metadata": {},
   "source": [
    "# Abkürzungen ausschreiben\n",
    "Da viele Twitter User Abkürzungen verwenden werden diese in ihre eigentlichen vollständigen Wörter umgewandelt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d7c3d4a-14f3-4c5d-bd9f-a6acfc1b403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_word = {\n",
    "    'AFAIK': 'As Far As I Know',\n",
    "    'AFK': 'Away From Keyboard',\n",
    "    'ASAP': 'As Soon As Possible',\n",
    "    'ATK': 'At The Keyboard',\n",
    "    'ATM': 'At The Moment',\n",
    "    'A3': 'Anytime, Anywhere, Anyplace',\n",
    "    'BAK': 'Back At Keyboard',\n",
    "    'BBL': 'Be Back Later',\n",
    "    'BBS': 'Be Back Soon',\n",
    "    'BFN': 'Bye For Now',\n",
    "    'B4N': 'Bye For Now',\n",
    "    'BRB': 'Be Right Back',\n",
    "    'BRT': 'Be Right There',\n",
    "    'BTW': 'By The Way',\n",
    "    'B4': 'Before',\n",
    "    'CU': 'See You',\n",
    "    'CUL8R': 'See You Later',\n",
    "    'CYA': 'See You',\n",
    "    'FAQ': 'Frequently Asked Questions',\n",
    "    'FC': 'Fingers Crossed',\n",
    "    'FWIW': \"For What It's Worth\",\n",
    "    'FYI': 'For Your Information',\n",
    "    'GAL': 'Get A Life',\n",
    "    'GG': 'Good Game',\n",
    "    'GN': 'Good Night',\n",
    "    'GMTA': 'Great Minds Think Alike',\n",
    "    'GR8': 'Great!',\n",
    "    'G9': 'Genius',\n",
    "    'IC': 'I See',\n",
    "    'ICQ': 'I Seek you (also a chat program)',\n",
    "    'ILU': 'ILU: I Love You',\n",
    "    'IMHO': 'In My Honest/Humble Opinion',\n",
    "    'IMO': 'In My Opinion',\n",
    "    'IOW': 'In Other Words',\n",
    "    'IRL': 'In Real Life',\n",
    "    'KISS': 'Keep It Simple, Stupid',\n",
    "    'LDR': 'Long Distance Relationship',\n",
    "    'LMAO': 'Laugh My A.. Off',\n",
    "    'LOL': 'Laughing Out Loud',\n",
    "    'LTNS': 'Long Time No See',\n",
    "    'L8R': 'Later',\n",
    "    'MTE': 'My Thoughts Exactly',\n",
    "    'M8': 'Mate',\n",
    "    'NRN': 'No Reply Necessary',\n",
    "    'OIC': 'Oh I See',\n",
    "    'PITA': 'Pain In The A..',\n",
    "    'PRT': 'Party',\n",
    "    'PRW': 'Parents Are Watching',\n",
    "    'QPSA?': 'Que Pasa?',\n",
    "    'ROFL': 'Rolling On The Floor Laughing',\n",
    "    'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
    "    'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off',\n",
    "    'SK8': 'Skate',\n",
    "    'STATS': 'Your sex and age',\n",
    "    'ASL': 'Age, Sex, Location',\n",
    "    'THX': 'Thank You',\n",
    "    'TTFN': 'Ta-Ta For Now!',\n",
    "    'TTYL': 'Talk To You Later',\n",
    "    'R': 'ARE',\n",
    "    'U': 'You',\n",
    "    'U2': 'You Too',\n",
    "    'U4E': 'Yours For Ever',\n",
    "    'WB': 'Welcome Back',\n",
    "    'WTF': 'What The F...',\n",
    "    'WTG': 'Way To Go!',\n",
    "    'WUF': 'Where Are You From?',\n",
    "    'W8': 'Wait...',\n",
    "    '7K': 'Sick:-D Laugher',\n",
    "    'TFW': 'That feeling when',\n",
    "    'MFW': 'My face when',\n",
    "    'MRW': 'My reaction when',\n",
    "    'IFYP': 'I feel your pain',\n",
    "    'TNTL': 'Trying not to laugh',\n",
    "    'JK': 'Just kidding',\n",
    "    'IDC': \"I don't care\",\n",
    "    'ILY': 'I love you',\n",
    "    'IMU': 'I miss you',\n",
    "    'ADIH': 'Another day in hell',\n",
    "    'ZZZ': 'Sleeping, bored, tired',\n",
    "    'WYWH': 'Wish you were here',\n",
    "    'TIME': 'Tears in my eyes',\n",
    "    'BAE': 'Before anyone else',\n",
    "    'FIMH': 'Forever in my heart',\n",
    "    'BSAAW': 'Big smile and a wink',\n",
    "    'BWL': 'Bursting with laughter',\n",
    "    'BFF': 'Best friends forever',\n",
    "    'CSL': \"Can't stop laughing\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86d9d720-860d-471c-bb2d-ff550dc15225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999      I LOVE @Health4UandPets You guys ARE the best!!\n",
       "800000    im meeting up with one of my besties tonight! ...\n",
       "800001    @DaRealSunisaKim Thanks for the Twitter add, S...\n",
       "800002    Being sick can be really cheap when it hurts t...\n",
       "800003       @LovesBrooklyn2 he has that effect on everyone\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def short_conv(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_word:\n",
    "            new_text.append(chat_word[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "data['text'] = data['text'].apply(lambda text: short_conv(text))\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b8d0d-45cf-4531-acf4-9fc107348a7a",
   "metadata": {},
   "source": [
    "# Spelling Correction\n",
    "Viele User vertippen sich oft, um mehr Konsistenz zu haben wird dies korrigiert.\n",
    "(Auskommentiert, da es extrem viel Leistung zieht und lange braucht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8f8bb89-9b83-4895-851c-cc747f7d50dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['text'] = data['text'].apply(lambda text: TextBlob(text).correct().string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20e528d5-624d-4d21-84c1-f1611d63e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0098cf-f49d-408c-a232-fe8459057477",
   "metadata": {},
   "source": [
    "# Text in Lowercase umwandeln\n",
    "Um Konsistenz zu gewährleisten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1b942d0-f008-4d10-ae9e-7c2c7644addd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999      i love @health4uandpets you guys are the best!!\n",
       "800000    im meeting up with one of my besties tonight! ...\n",
       "800001    @darealsunisakim thanks for the twitter add, s...\n",
       "800002    being sick can be really cheap when it hurts t...\n",
       "800003       @lovesbrooklyn2 he has that effect on everyone\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']=data['text'].str.lower()\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebc237a-5dbb-45c6-bfb0-890a0a09600a",
   "metadata": {},
   "source": [
    "# Stopwords entfernen\n",
    "Stopword sind Wörter wie \"and\", \"the\" oder \"is\" und tragen oft keine Bedeutung zur Sentimentanalyse bei. Daher werden sie entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80c255da-34be-4cc4-9f26-bcf1489d1893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edc4398d-0cf2-4db9-9553-541f7ac1b6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999                    love @health4uandpets guys best!!\n",
       "800000    im meeting one besties tonight! cant wait!! - ...\n",
       "800001    @darealsunisakim thanks twitter add, sunisa! g...\n",
       "800002    sick really cheap hurts much eat real food plu...\n",
       "800003                      @lovesbrooklyn2 effect everyone\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "data['text'] = data['text'].apply(lambda text: cleaning_stopwords(text))\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729e5735-5f12-4ab3-acf4-f97de8ef0d1b",
   "metadata": {},
   "source": [
    "# URLs, Benutzernamen, Sonderzeichen und Zahlen entfernen\n",
    "Tweets enthalten oft Links, Erwähnungen, Zahlen oder Sonderzeichen die für uns irrelevant sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17cdb69b-dfad-4fb2-aa6e-d36fee1bffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "userPattern = r\"@[^\\s]+\"\n",
    "#hastagPattern = r\"#[^\\s]+\"\n",
    "alphaPattern = r\"[^a-zA-Z]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b2fbda4-df52-4158-928f-bb3fb66b8e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999                                    love  guys best  \n",
       "800000    im meeting one besties tonight  cant wait     ...\n",
       "800001     thanks twitter add  sunisa  got meet hin show...\n",
       "800002    sick really cheap hurts much eat real food plu...\n",
       "800003                                      effect everyone\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(lambda text: re.sub(userPattern,'', text))\n",
    "#data['text'] = data['text'].apply(lambda text: re.sub(hastagPattern,'',text))\n",
    "data['text'] = data['text'].apply(lambda text: re.sub(urlPattern,'',text))\n",
    "data['text'] = data['text'].apply(lambda text: re.sub(alphaPattern,' ', text))\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc159eec-a39b-4f69-89eb-1fe143b6c7f6",
   "metadata": {},
   "source": [
    "Sonderzeichen entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd5e7ec7-0b96-40e2-886b-70ec4e3c6cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72ce048b-8194-4033-8a42-1bcafaf0fa0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999                                    love  guys best  \n",
       "800000    im meeting one besties tonight  cant wait     ...\n",
       "800001     thanks twitter add  sunisa  got meet hin show...\n",
       "800002    sick really cheap hurts much eat real food plu...\n",
       "800003                                      effect everyone\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']= data['text'].apply(lambda x: cleaning_punctuations(x))\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75719127-3d24-40dc-9cb9-201ddbad2248",
   "metadata": {},
   "source": [
    "# Sequenzen von mehr als 3 gleichen Buchstaben kürzen\n",
    "Sequenzen von mehr als 3 gleichen Buchstaben sind oft Tippfehler oder es wird \"Slang\" verwendet. Für mehr Konsistenz werden Sequenzen von mehr als 3 Buchstaben auf 2 gekürzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a83f278a-8183-4352-b767-9f0302fe4545",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencePattern = r\"(.)\\1\\1+\"\n",
    "seqReplacePattern = r\"\\1\\1\"\n",
    "def cleaning_repeating_char(text):\n",
    "    return re.sub(sequencePattern, seqReplacePattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6179e73-3991-4083-b5a2-17c36806a4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999                                    love  guys best  \n",
       "800000    im meeting one besties tonight  cant wait  gir...\n",
       "800001     thanks twitter add  sunisa  got meet hin show...\n",
       "800002    sick really cheap hurts much eat real food plu...\n",
       "800003                                      effect everyone\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(lambda x: cleaning_repeating_char(x))\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e98f4b8-ba1e-4bac-9551-43a4ed91f83c",
   "metadata": {},
   "source": [
    "# Sätze in Listen umwandeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25eddf59-241e-4fb2-a5ba-69c7a9453521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999                                   [love, guys, best]\n",
       "800000    [im, meeting, one, besties, tonight, cant, wai...\n",
       "800001    [thanks, twitter, add, sunisa, got, meet, hin,...\n",
       "800002    [sick, really, cheap, hurts, much, eat, real, ...\n",
       "800003                                   [effect, everyone]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data['text'] = data['text'].apply(tokenizer.tokenize)\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc7ccbc-b822-4373-b14c-b0ce2ef637b2",
   "metadata": {},
   "source": [
    "# STEMMING\n",
    "Wörter werden auf \"Stammformen\" gekürzt. (Wird im Projekt nicht verwendet, da wir uns für Lemmatization entschieden haben)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "378d82ee-ae96-47d1-8165-6adf60a8feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "\n",
    "#data['text']= data['text'].apply(lambda x: stemming_on_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c233859-7578-4dfb-bf51-61789241e763",
   "metadata": {},
   "source": [
    "# LEMMATIZATION\n",
    "Wörter werden in ihre Grundform zurückgeführt (z. B. \"running\" → \"run\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "29314d44-4165-4339-9470-999478826fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999                                    [love, guy, best]\n",
       "800000    [im, meeting, one, besties, tonight, cant, wai...\n",
       "800001    [thanks, twitter, add, sunisa, get, meet, hin,...\n",
       "800002    [sick, really, cheap, hurt, much, eat, real, f...\n",
       "800003                                   [effect, everyone]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Funktion zur Konvertierung von NLTK-POS-Tags zu WordNet-Tags\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # Standardmäßig Substantiv\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word, pos=get_wordnet_pos(word)) for word in data]\n",
    "    return text\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: lemmatizer_on_text(x))\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea45c9-1717-4f1a-94be-4e8cb623a169",
   "metadata": {},
   "source": [
    "# Daten vorbereiten\n",
    "Daten für das Modell in numerische Sequenzen umwandeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f8be885-a8f4-4658-9b91-e7dc8d1ba035",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.text\n",
    "y=data.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e92e25b-69b4-429b-8951-d56c0fdd4c61",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "Wir verwenden den Tokenizer, um Wörter in numerische Indizes zu kodieren, und padden die Sequenzen auf eine einheitliche Länge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f1f690d-af04-4603-abf7-4455687bc660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 500)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 500\n",
    "tok = Tokenizer(num_words=5000)\n",
    "tok.fit_on_texts(X)\n",
    "sequences = tok.texts_to_sequences(X)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "sequences_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ba88b8-5a3c-40d3-a53e-d8bddfdea9c2",
   "metadata": {},
   "source": [
    "### Datenset in Training/Validation und Test Datenset aufteilen (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6b511e39-5b4c-4ff4-8007-7ce8de3de64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(sequences_matrix, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22306f4d-892f-458c-8e6d-8020ba2f8bf2",
   "metadata": {},
   "source": [
    "# Model\n",
    "Das LSTM-Modell ist speziell für sequentielle Daten wie Text geeignet. Es kann sich Kontextinformationen aus früheren Wörtern merken.\n",
    "Das Modell besteht aus mehreren Schichten:\n",
    "\n",
    "### Embedding-Schicht\n",
    "Die Embedding-Schicht wandelt Wörter in dichte, numerische Vektoren um, die semantische Informationen enthalten.\n",
    "Diese Transformation hilft, die Beziehungen zwischen Wörtern (z. B. Synonyme oder ähnliche Kontexte) mathematisch darzustellen.\n",
    "Parameter:\n",
    "input_dim=5000: Die Anzahl der häufigsten Wörter im Vokabular.\n",
    "output_dim=75: Die Dimension der Vektorrepräsentation für jedes Wort.\n",
    "\n",
    "### LSTM-Schichten\n",
    "LSTM-Schichten ermöglichen es dem Modell, sich an frühere Informationen in einer Sequenz zu erinnern und dabei irrelevante Details zu \"vergessen\".\n",
    "\n",
    "Tweets sind sequenzielle Daten, bei denen der Kontext (z. B. die Bedeutung eines Wortes basierend auf vorherigen Wörtern) entscheidend ist.\n",
    "\n",
    "Parameter:\n",
    "units=64: Die Anzahl der Einheiten (Neuronen) in der LSTM-Schicht.\n",
    "recurrent_dropout=0.2: Dropout für rekurrente Verbindungen (zwischen den Zeitschritten der Sequenz).\n",
    "dropout=0.5: Dropout für Verbindungen zwischen den Neuronen in der Schicht.\n",
    "\n",
    "### Dense-Schicht\n",
    "Eine vollständig verbundene Schicht, die die gelernten Merkmale in ein repräsentatives Format umwandelt, bevor sie zur Klassifikation verwendet werden. Diese Schicht ermöglicht eine zusätzliche nicht-lineare Transformation der Daten.\n",
    "\n",
    "### Batch-Normalisierung\n",
    "Normiert die Ausgabe der vorherigen Schicht, um den Lernprozess zu stabilisieren und zu beschleunigen.\n",
    "\n",
    "### Dropout-Schichten\n",
    "Deaktiviert während des Trainings zufällig eine bestimmte Anzahl von Neuronen in einer Schicht.\n",
    "Dropout hilft, Overfitting zu vermeiden, indem das Modell gezwungen wird, robuste Merkmale zu lernen, die nicht von einer bestimmten Teilmenge von Neuronen abhängen.\n",
    "\n",
    "### Ausgabeschicht (Dense)\n",
    "Eine Sigmoid-Aktivierungsfunktion wird verwendet, um die Wahrscheinlichkeiten für die binäre Klassifikation (positiv/negativ) auszugeben. Die Sigmoid-Funktion beschränkt die Ausgabe auf einen Bereich zwischen 0 und 1, was ideal für binäre Klassifikationen ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38236efb-c625-4038-abf9-4661a1e07491",
   "metadata": {},
   "source": [
    "\n",
    "Optimizer: Adam (Adaptive Moment Estimation) für schnelle Konvergenz.\n",
    "\n",
    "Verlustfunktion: Binary Crossentropy, geeignet für binäre Klassifikationsaufgaben.\n",
    "\n",
    "Metrik: Genauigkeit (Accuracy), um die Leistung des Modells zu bewerten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1bcacded-ee73-46c7-a299-262527092650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000,75,input_length=max_len))\n",
    "\n",
    "# Add the first LSTM layer\n",
    "model.add(LSTM(64, return_sequences=True,recurrent_dropout=0.2,dropout=0.4))  # Return sequences to pass to the next LSTM layer\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "# Add a third LSTM layer\n",
    "model.add(LSTM(64,recurrent_dropout=0.3,dropout=0.3))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Add a fully connected layer\n",
    "model.add(Dense(64, activation='relu'))  # Intermediate dense layer for more learning capacity\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))  # Dropout for dense layer\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # For classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',  # Use sparse if labels are integers\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b205f-191b-4778-9a12-d4d79d6b8f89",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "Das Modell lernt aus den Trainingsdaten, während die Validierungsdaten dazu dienen, Overfitting zu verhindern.\n",
    "Wir verwenden EarlyStopping, um das Training zu beenden, wenn die Validierungsleistung nicht mehr besser wird und verwenden ReduceLROnPlateau um die Lernrate sinken zu lassen, falls wir uns auf einem Plateau befinden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3d5ca050-fabb-4dc7-9c44-e1fc0bb13fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = EarlyStopping(monitor='val_accuracy',patience = 3 ,mode='max')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.00001)\n",
    "callbacks_lst = [checkpoint,reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa702ed-7d77-4e26-a414-669d5bc3c8bc",
   "metadata": {},
   "source": [
    "# MODEL TRAINING\n",
    "mit einer Batchsize von 128 und 15 Epochen. Aus dem Trainingsdatenset werden 20% für die Validierung reserviert und dann verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9efcdbb9-729b-4bfa-8325-e58117208c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training on GPU...\") if tensorflow.config.list_physical_devices('GPU') else print(\"Training on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322c382-60ed-41c9-ba46-65e35f8159b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1083s\u001b[0m 1s/step - accuracy: 0.6560 - loss: 0.6013 - val_accuracy: 0.7592 - val_loss: 0.4925 - learning_rate: 0.0010\n",
      "Epoch 2/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1128s\u001b[0m 1s/step - accuracy: 0.7705 - loss: 0.4884 - val_accuracy: 0.7678 - val_loss: 0.4819 - learning_rate: 0.0010\n",
      "Epoch 3/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1134s\u001b[0m 1s/step - accuracy: 0.7786 - loss: 0.4716 - val_accuracy: 0.7688 - val_loss: 0.4786 - learning_rate: 0.0010\n",
      "Epoch 4/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1135s\u001b[0m 1s/step - accuracy: 0.7862 - loss: 0.4584 - val_accuracy: 0.7735 - val_loss: 0.4805 - learning_rate: 0.0010\n",
      "Epoch 5/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1137s\u001b[0m 1s/step - accuracy: 0.7899 - loss: 0.4473 - val_accuracy: 0.7683 - val_loss: 0.4862 - learning_rate: 0.0010\n",
      "Epoch 6/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1134s\u001b[0m 1s/step - accuracy: 0.7989 - loss: 0.4319 - val_accuracy: 0.7729 - val_loss: 0.4867 - learning_rate: 5.0000e-04\n",
      "Epoch 7/15\n",
      "\u001b[1m 979/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.8027 - loss: 0.4259"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,Y_train,batch_size=128,epochs=15, validation_split=0.2, callbacks=callbacks_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e731542d-e4d0-461d-a86d-cc9700be3e5a",
   "metadata": {},
   "source": [
    "# Ergebnisse visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1064819-1b3d-48e4-ac49-7a440c81fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig_loss = plt.figure()\n",
    "plt.plot(history.history['loss'], color='teal', label='loss')\n",
    "plt.plot(history.history['val_loss'], color='orange', label='val_loss')\n",
    "fig_loss.suptitle('Loss', fontsize=20)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc84ce38-852c-452c-99b8-11cabadfe3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig_accuracy = plt.figure()\n",
    "plt.plot(history.history['accuracy'], color='blue', label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], color='green', label='val_accuracy')\n",
    "fig_accuracy.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92441d5-e00f-4e74-9ad0-399bf8deca86",
   "metadata": {},
   "source": [
    "# Evaluierung des Models anhand der Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72866b8-009e-4b23-a335-f5264271664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ec7939-e137-491e-83c6-c854df531f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb992a-a51b-4d35-adee-ac4ca36f9137",
   "metadata": {},
   "source": [
    "### Speichern des Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0193580-f1d3-4317-a15c-8b39f530a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('testing4.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c8bb8b-ce7c-4812-be93-5cdad50b0004",
   "metadata": {},
   "source": [
    "# Explainable AI (LIME)\n",
    "LIME ist ein Ansatz zur Erklärbarkeit von Modellen (Explainable AI, XAI), der entwickelt wurde, um Vorhersagen von Machine-Learning-Modellen interpretierbar und nachvollziehbar zu machen. Es bietet verständliche, lokale Erklärungen für individuelle Vorhersagen, unabhängig davon, wie komplex oder undurchsichtig das zugrunde liegende Modell ist.\n",
    "\n",
    "### LimeTextExplainer\n",
    "LimeTextExplainer ist eine spezialisierte Implementierung von LIME für die Verarbeitung von Textdaten. Es erklärt, welche Wörter oder Phrasen in einem Text am stärksten zur Vorhersage eines Modells beigetragen haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12729116-9ddd-45d7-a0ba-9054634a9c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=['negative', 'positive'])\n",
    "model_lime = tensorflow.keras.models.load_model('testing1.keras')\n",
    "print(model.inputs)\n",
    "print(model.outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7464cf-77d7-47b5-a0f3-99dbcd6e5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(texts):\n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        text = short_conv(text)\n",
    "        text = text.lower()\n",
    "        text = cleaning_stopwords(text)\n",
    "        text = re.sub(userPattern,'', text)\n",
    "        text = re.sub(urlPattern,'', text)\n",
    "        text = re.sub(alphaPattern,' ', text)\n",
    "        text = cleaning_punctuations(text)\n",
    "        text = cleaning_repeating_char(text)\n",
    "        text = tokenizer.tokenize(text)\n",
    "        text = lemmatizer_on_text(text)\n",
    "        cleaned_texts.append(text)\n",
    "    tokenized_texts = tok.texts_to_sequences(cleaned_texts)\n",
    "    padded_texts = sequence.pad_sequences(tokenized_texts, maxlen=500)\n",
    "    del cleaned_texts\n",
    "    return padded_texts\n",
    "\n",
    "def predict_proba(texts):\n",
    "    processed_texts = preprocess_texts(texts)  # Stellen Sie sicher, dass Texte verarbeitet werden\n",
    "    probabilities = model_lime.predict(processed_texts)  # Gibt eine Wahrscheinlichkeit pro Text\n",
    "    # Ergänzen der Wahrscheinlichkeit für die andere Klasse\n",
    "    probabilities = np.hstack((1 - probabilities, probabilities))\n",
    "    return probabilities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe45b28-1237-45fc-b98e-6691fe368e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"This product is insane, keep the good work up @Logitech. I LOVE IT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb668b2-8e71-45b3-8b14-1de0dcc8a19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = explainer.explain_instance(\n",
    "    example_text,\n",
    "    predict_proba,\n",
    "    num_features=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da88d6e8-9491-4da3-979b-7ffe5a70c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82104ff-3221-43c5-8904-09b20b378d91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
