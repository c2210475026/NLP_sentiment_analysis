{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ed570a02eb2e38",
   "metadata": {},
   "source": [
    "# Motivation and State of the Art\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95921a82e61ecf6e",
   "metadata": {},
   "source": [
    "# Installing libraries\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27679da-b5dc-4a33-91ac-24fa89058f23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (4.3.3)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
      "Requirement already satisfied: tensorflow in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: keras>=3.5.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: packaging in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: setuptools in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (24.12.23)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (1.68.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (5.29.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: rich in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: optree in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: namex in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: numpy in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: keras in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (3.7.0)\n",
      "Requirement already satisfied: optree in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from keras) (0.13.1)\n",
      "Requirement already satisfied: absl-py in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: namex in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from keras) (3.12.1)\n",
      "Requirement already satisfied: rich in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: ml-dtypes in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: numpy in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: packaging in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from keras) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from optree->keras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: pandas in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: nltk in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: joblib in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: click in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: seaborn in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: pandas>=1.2 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\n",
      "Requirement already satisfied: pillow>=8 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Requirement already satisfied: mlxtend in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (0.23.3)\n",
      "Requirement already satisfied: scikit-learn>=1.3.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from mlxtend) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.16.2 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from mlxtend) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from mlxtend) (1.13.1)\n",
      "Requirement already satisfied: joblib>=0.13.2 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from mlxtend) (1.4.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from mlxtend) (3.10.0)\n",
      "Requirement already satisfied: pandas>=0.24.2 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from mlxtend) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from matplotlib>=3.0.0->mlxtend) (24.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.1)\n",
      "Requirement already satisfied: pillow>=8 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from matplotlib>=3.0.0->mlxtend) (11.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from matplotlib>=3.0.0->mlxtend) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from matplotlib>=3.0.0->mlxtend) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.8)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from pandas>=0.24.2->mlxtend) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from pandas>=0.24.2->mlxtend) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from scikit-learn>=1.3.1->mlxtend) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n",
      "Requirement already satisfied: textblob in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: tqdm in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from nltk>=3.8->textblob) (4.67.1)\n",
      "Requirement already satisfied: click in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from nltk>=3.8->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./NLP_sentiment_analysis_environment/lib/python3.10/site-packages (from nltk>=3.8->textblob) (2024.11.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim --upgrade\n",
    "!pip install tensorflow\n",
    "!pip install numpy\n",
    "!pip install keras --upgrade\n",
    "!pip install pandas --upgrade\n",
    "!pip install nltk\n",
    "!pip install seaborn\n",
    "!pip install mlxtend\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c10d904-83f9-4452-bba5-f8137eeb08f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/c2210475029/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/c2210475029/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2025-01-04 17:18:31.279432: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736011111.297696  554666 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736011111.303268  554666 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-04 17:18:31.321229: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import rcParams\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import string\n",
    "import tensorflow\n",
    "from sklearn.metrics  import classification_report ,confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0129ea4a06e0bf",
   "metadata": {},
   "source": [
    "# Get the data\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ca1fce-dd30-48b6-9d6e-85e9bcd22615",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", engine=\"python\")\n",
    "data.columns = [\"label\", \"id\", \"date\", \"query\", \"username\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30e35526-f0c0-421e-908d-342a937c7b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label          id                          date     query       username  \\\n",
       "0      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
       "1      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
       "2      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
       "3      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
       "4      0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
       "\n",
       "                                                text  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d697c86513c5b60a",
   "metadata": {},
   "source": [
    "Only the 'text' and 'label' columns are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad0a007c-0550-45f4-a58b-e36c0a1c6067",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[['text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81bde87c-c2c6-4774-8313-581dcce5a80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  is upset that he can't update his Facebook by ...      0\n",
       "1  @Kenichan I dived many times for the ball. Man...      0\n",
       "2    my whole body feels itchy and like its on fire       0\n",
       "3  @nationwideclass no, it's not behaving at all....      0\n",
       "4                      @Kwesidei not the whole crew       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "124e4c3d-dc65-4287-b153-52df66a22586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3fcf99b-b0a8-4a17-b205-1da881f6ebbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length: 1599999\n"
     ]
    }
   ],
   "source": [
    "print('data length:', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cec1941c-65f4-4557-8ba8-079a7fcaf531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58617553-5304-406e-a193-412556b4f9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                       text  label\n",
       "0        is upset that he can't update his Facebook by ...      0\n",
       "1        @Kenichan I dived many times for the ball. Man...      0\n",
       "2          my whole body feels itchy and like its on fire       0\n",
       "3        @nationwideclass no, it's not behaving at all....      0\n",
       "4                            @Kwesidei not the whole crew       0\n",
       "...                                                    ...    ...\n",
       "1599994  Just woke up. Having no school is the best fee...      4\n",
       "1599995  TheWDB.com - Very cool to hear old Walt interv...      4\n",
       "1599996  Are you ready for your MoJo Makeover? Ask me f...      4\n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! ...      4\n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity...      4\n",
       "\n",
       "[1599999 rows x 2 columns]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd6c53ab-53f9-472b-adfd-f382902f93c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     object\n",
       "label     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b184b90fda23e69",
   "metadata": {},
   "source": [
    "Label '4' gets changed to label '1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f43d7a6e-908c-488d-abdf-6964133a9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['label']==4, 'label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "173bbc6f-6890-49d6-b5bf-b1c2ae82c771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                       text  label\n",
       "0        is upset that he can't update his Facebook by ...      0\n",
       "1        @Kenichan I dived many times for the ball. Man...      0\n",
       "2          my whole body feels itchy and like its on fire       0\n",
       "3        @nationwideclass no, it's not behaving at all....      0\n",
       "4                            @Kwesidei not the whole crew       0\n",
       "...                                                    ...    ...\n",
       "1599994  Just woke up. Having no school is the best fee...      1\n",
       "1599995  TheWDB.com - Very cool to hear old Walt interv...      1\n",
       "1599996  Are you ready for your MoJo Makeover? Ask me f...      1\n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! ...      1\n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity...      1\n",
       "\n",
       "[1599999 rows x 2 columns]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b625f3dd-9f79-48ac-bda1-81e5272525df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    800000\n",
       "0    799999\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ea0141a-9939-4333-a512-d24b17ee5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data[data['label'] == 1]\n",
    "data_neg = data[data['label'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f307dcb-84e9-4c7e-ba6c-62dbebbe4522",
   "metadata": {},
   "source": [
    "## Dataset reduction for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b204e3f4-484a-4d51-9a42-a7ed6936b0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    800000\n",
       "0    799999\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pos = data_pos.iloc[:int(70000)]\n",
    "data_neg = data_neg.iloc[:int(70000)]\n",
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32e96fa1-9152-4acf-8f5b-dd047c09fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_pos, data_neg])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f9d09-f8d7-41ad-89df-17fc7b909d2d",
   "metadata": {},
   "source": [
    "# Text cleaning\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60fe4fc9-8564-43ba-b316-42172c20d55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                      text  label\n",
       "799999       I LOVE @Health4UandPets u guys r the best!!       1\n",
       "800000  im meeting up with one of my besties tonight! ...      1\n",
       "800001  @DaRealSunisaKim Thanks for the Twitter add, S...      1\n",
       "800002  Being sick can be really cheap when it hurts t...      1\n",
       "800003    @LovesBrooklyn2 he has that effect on everyone       1\n",
       "...                                                   ...    ...\n",
       "69995                                            Wow....       0\n",
       "69996   @Dainfam0uszd  NO UR NOT.....U R MY ONLY P.E.O...      0\n",
       "69997   I will be so much happier when I don't have to...      0\n",
       "69998   @mariancall buy it and bring it up here I woul...      0\n",
       "69999   @jenniferrr_gee we haven't had it yet  he's 't...      0\n",
       "\n",
       "[140000 rows x 2 columns]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1368a94-c77b-4f5a-b663-9ea3c2206769",
   "metadata": {},
   "source": [
    "Abbreviations are changed to whole words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d7c3d4a-14f3-4c5d-bd9f-a6acfc1b403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_word = {\n",
    "    'AFAIK': 'As Far As I Know',\n",
    "    'AFK': 'Away From Keyboard',\n",
    "    'ASAP': 'As Soon As Possible',\n",
    "    'ATK': 'At The Keyboard',\n",
    "    'ATM': 'At The Moment',\n",
    "    'A3': 'Anytime, Anywhere, Anyplace',\n",
    "    'BAK': 'Back At Keyboard',\n",
    "    'BBL': 'Be Back Later',\n",
    "    'BBS': 'Be Back Soon',\n",
    "    'BFN': 'Bye For Now',\n",
    "    'B4N': 'Bye For Now',\n",
    "    'BRB': 'Be Right Back',\n",
    "    'BRT': 'Be Right There',\n",
    "    'BTW': 'By The Way',\n",
    "    'B4': 'Before',\n",
    "    'CU': 'See You',\n",
    "    'CUL8R': 'See You Later',\n",
    "    'CYA': 'See You',\n",
    "    'FAQ': 'Frequently Asked Questions',\n",
    "    'FC': 'Fingers Crossed',\n",
    "    'FWIW': \"For What It's Worth\",\n",
    "    'FYI': 'For Your Information',\n",
    "    'GAL': 'Get A Life',\n",
    "    'GG': 'Good Game',\n",
    "    'GN': 'Good Night',\n",
    "    'GMTA': 'Great Minds Think Alike',\n",
    "    'GR8': 'Great!',\n",
    "    'G9': 'Genius',\n",
    "    'IC': 'I See',\n",
    "    'ICQ': 'I Seek you (also a chat program)',\n",
    "    'ILU': 'ILU: I Love You',\n",
    "    'IMHO': 'In My Honest/Humble Opinion',\n",
    "    'IMO': 'In My Opinion',\n",
    "    'IOW': 'In Other Words',\n",
    "    'IRL': 'In Real Life',\n",
    "    'KISS': 'Keep It Simple, Stupid',\n",
    "    'LDR': 'Long Distance Relationship',\n",
    "    'LMAO': 'Laugh My A.. Off',\n",
    "    'LOL': 'Laughing Out Loud',\n",
    "    'LTNS': 'Long Time No See',\n",
    "    'L8R': 'Later',\n",
    "    'MTE': 'My Thoughts Exactly',\n",
    "    'M8': 'Mate',\n",
    "    'NRN': 'No Reply Necessary',\n",
    "    'OIC': 'Oh I See',\n",
    "    'PITA': 'Pain In The A..',\n",
    "    'PRT': 'Party',\n",
    "    'PRW': 'Parents Are Watching',\n",
    "    'QPSA?': 'Que Pasa?',\n",
    "    'ROFL': 'Rolling On The Floor Laughing',\n",
    "    'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
    "    'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off',\n",
    "    'SK8': 'Skate',\n",
    "    'STATS': 'Your sex and age',\n",
    "    'ASL': 'Age, Sex, Location',\n",
    "    'THX': 'Thank You',\n",
    "    'TTFN': 'Ta-Ta For Now!',\n",
    "    'TTYL': 'Talk To You Later',\n",
    "    'R': 'ARE',\n",
    "    'U': 'You',\n",
    "    'U2': 'You Too',\n",
    "    'U4E': 'Yours For Ever',\n",
    "    'WB': 'Welcome Back',\n",
    "    'WTF': 'What The F...',\n",
    "    'WTG': 'Way To Go!',\n",
    "    'WUF': 'Where Are You From?',\n",
    "    'W8': 'Wait...',\n",
    "    '7K': 'Sick:-D Laugher',\n",
    "    'TFW': 'That feeling when',\n",
    "    'MFW': 'My face when',\n",
    "    'MRW': 'My reaction when',\n",
    "    'IFYP': 'I feel your pain',\n",
    "    'TNTL': 'Trying not to laugh',\n",
    "    'JK': 'Just kidding',\n",
    "    'IDC': \"I don't care\",\n",
    "    'ILY': 'I love you',\n",
    "    'IMU': 'I miss you',\n",
    "    'ADIH': 'Another day in hell',\n",
    "    'ZZZ': 'Sleeping, bored, tired',\n",
    "    'WYWH': 'Wish you were here',\n",
    "    'TIME': 'Tears in my eyes',\n",
    "    'BAE': 'Before anyone else',\n",
    "    'FIMH': 'Forever in my heart',\n",
    "    'BSAAW': 'Big smile and a wink',\n",
    "    'BWL': 'Bursting with laughter',\n",
    "    'BFF': 'Best friends forever',\n",
    "    'CSL': \"Can't stop laughing\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86d9d720-860d-471c-bb2d-ff550dc15225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999      I LOVE @Health4UandPets You guys ARE the best!!\n",
       "800000    im meeting up with one of my besties tonight! ...\n",
       "800001    @DaRealSunisaKim Thanks for the Twitter add, S...\n",
       "800002    Being sick can be really cheap when it hurts t...\n",
       "800003       @LovesBrooklyn2 he has that effect on everyone\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def short_conv(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_word:\n",
    "            new_text.append(chat_word[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "data['text'] = data['text'].apply(lambda text: short_conv(text))\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b8d0d-45cf-4531-acf4-9fc107348a7a",
   "metadata": {},
   "source": [
    "Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8f8bb89-9b83-4895-851c-cc747f7d50dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['text'] = data['text'].apply(lambda text: TextBlob(text).correct().string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20e528d5-624d-4d21-84c1-f1611d63e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0098cf-f49d-408c-a232-fe8459057477",
   "metadata": {},
   "source": [
    "LOWERCASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1b942d0-f008-4d10-ae9e-7c2c7644addd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999      i love @health4uandpets you guys are the best!!\n",
       "800000    im meeting up with one of my besties tonight! ...\n",
       "800001    @darealsunisakim thanks for the twitter add, s...\n",
       "800002    being sick can be really cheap when it hurts t...\n",
       "800003       @lovesbrooklyn2 he has that effect on everyone\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']=data['text'].str.lower()\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80c255da-34be-4cc4-9f26-bcf1489d1893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edc4398d-0cf2-4db9-9553-541f7ac1b6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999                    love @health4uandpets guys best!!\n",
       "800000    im meeting one besties tonight! cant wait!! - ...\n",
       "800001    @darealsunisakim thanks twitter add, sunisa! g...\n",
       "800002    sick really cheap hurts much eat real food plu...\n",
       "800003                      @lovesbrooklyn2 effect everyone\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "data['text'] = data['text'].apply(lambda text: cleaning_stopwords(text))\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729e5735-5f12-4ab3-acf4-f97de8ef0d1b",
   "metadata": {},
   "source": [
    "REMOVING USERNAME, URL, HASHTAGS and Non-Alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17cdb69b-dfad-4fb2-aa6e-d36fee1bffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "userPattern = r\"@[^\\s]+\"\n",
    "hastagPattern = r\"#[^\\s]+\"\n",
    "alphaPattern = r\"[^a-zA-Z]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b2fbda4-df52-4158-928f-bb3fb66b8e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999                                    love  guys best  \n",
       "800000    im meeting one besties tonight  cant wait     ...\n",
       "800001     thanks twitter add  sunisa  got meet hin show...\n",
       "800002    sick really cheap hurts much eat real food plu...\n",
       "800003                                      effect everyone\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(lambda text: re.sub(userPattern,'', text))\n",
    "data['text'] = data['text'].apply(lambda text: re.sub(hastagPattern,'',text))\n",
    "data['text'] = data['text'].apply(lambda text: re.sub(urlPattern,'',text))\n",
    "data['text'] = data['text'].apply(lambda text: re.sub(alphaPattern,' ', text))\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc159eec-a39b-4f69-89eb-1fe143b6c7f6",
   "metadata": {},
   "source": [
    "REMOVE PUNCTUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd5e7ec7-0b96-40e2-886b-70ec4e3c6cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72ce048b-8194-4033-8a42-1bcafaf0fa0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999                                    love  guys best  \n",
       "800000    im meeting one besties tonight  cant wait     ...\n",
       "800001     thanks twitter add  sunisa  got meet hin show...\n",
       "800002    sick really cheap hurts much eat real food plu...\n",
       "800003                                      effect everyone\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']= data['text'].apply(lambda x: cleaning_punctuations(x))\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75719127-3d24-40dc-9cb9-201ddbad2248",
   "metadata": {},
   "source": [
    "Removing Sequences of 3 or more Identical Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a83f278a-8183-4352-b767-9f0302fe4545",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencePattern = r\"(.)\\1\\1+\"\n",
    "seqReplacePattern = r\"\\1\\1\"\n",
    "def cleaning_repeating_char(text):\n",
    "    return re.sub(sequencePattern, seqReplacePattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6179e73-3991-4083-b5a2-17c36806a4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999                                    love  guys best  \n",
       "800000    im meeting one besties tonight  cant wait  gir...\n",
       "800001     thanks twitter add  sunisa  got meet hin show...\n",
       "800002    sick really cheap hurts much eat real food plu...\n",
       "800003                                      effect everyone\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(lambda x: cleaning_repeating_char(x))\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e98f4b8-ba1e-4bac-9551-43a4ed91f83c",
   "metadata": {},
   "source": [
    "TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25eddf59-241e-4fb2-a5ba-69c7a9453521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999                                   [love, guys, best]\n",
       "800000    [im, meeting, one, besties, tonight, cant, wai...\n",
       "800001    [thanks, twitter, add, sunisa, got, meet, hin,...\n",
       "800002    [sick, really, cheap, hurts, much, eat, real, ...\n",
       "800003                                   [effect, everyone]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data['text'] = data['text'].apply(tokenizer.tokenize)\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc7ccbc-b822-4373-b14c-b0ce2ef637b2",
   "metadata": {},
   "source": [
    "STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "378d82ee-ae96-47d1-8165-6adf60a8feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "\n",
    "data['text']= data['text'].apply(lambda x: stemming_on_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c233859-7578-4dfb-bf51-61789241e763",
   "metadata": {},
   "source": [
    "LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29314d44-4165-4339-9470-999478826fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999                                   [love, guys, best]\n",
       "800000    [im, meeting, one, besties, tonight, cant, wai...\n",
       "800001    [thanks, twitter, add, sunisa, got, meet, hin,...\n",
       "800002    [sick, really, cheap, hurts, much, eat, real, ...\n",
       "800003                                   [effect, everyone]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: lemmatizer_on_text(x))\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea45c9-1717-4f1a-94be-4e8cb623a169",
   "metadata": {},
   "source": [
    "CREATING TRAINING,VALIDATION SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f8be885-a8f4-4658-9b91-e7dc8d1ba035",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.text\n",
    "y=data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f1f690d-af04-4603-abf7-4455687bc660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000, 500)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 500\n",
    "tok = Tokenizer(num_words=2000)\n",
    "tok.fit_on_texts(X)\n",
    "sequences = tok.texts_to_sequences(X)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "sequences_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b511e39-5b4c-4ff4-8007-7ce8de3de64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(sequences_matrix, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22306f4d-892f-458c-8e6d-8020ba2f8bf2",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bcacded-ee73-46c7-a299-262527092650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nvme/home/c2210475029/NLP_sentiment_analysis/NLP_sentiment_analysis_environment/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1736011149.234699  554666 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(2000,50,input_length=max_len))\n",
    "\n",
    "# Add the first LSTM layer\n",
    "model.add(LSTM(100, return_sequences=True,recurrent_dropout=0.2,dropout=0.2))  # Return sequences to pass to the next LSTM layer\n",
    "model.add(Dropout(0.3))  # Dropout to prevent overfitting\n",
    "\n",
    "# Add a second LSTM layer\n",
    "model.add(LSTM(100, return_sequences=True,recurrent_dropout=0.2,dropout=0.2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Add a third LSTM layer\n",
    "model.add(LSTM(100,recurrent_dropout=0.2,dropout=0.2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Add a fully connected layer\n",
    "model.add(Dense(64, activation='relu'))  # Intermediate dense layer for more learning capacity\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))  # Dropout for dense layer\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # For classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',  # Use sparse if labels are integers\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d5ca050-fabb-4dc7-9c44-e1fc0bb13fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = EarlyStopping(monitor='val_accuracy',patience = 10 ,mode='max')\n",
    "callbacks_lst = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa702ed-7d77-4e26-a414-669d5bc3c8bc",
   "metadata": {},
   "source": [
    "MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9efcdbb9-729b-4bfa-8325-e58117208c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training on GPU...\") if tensorflow.config.list_physical_devices('GPU') else print(\"Training on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322c382-60ed-41c9-ba46-65e35f8159b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m197/690\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15:23\u001b[0m 2s/step - accuracy: 0.5357 - loss: 0.6895"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,Y_train,batch_size=128,epochs=10, validation_split=0.1, callbacks=callbacks_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1064819-1b3d-48e4-ac49-7a440c81fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig_loss = plt.figure()\n",
    "plt.plot(history.history['loss'], color='teal', label='loss')\n",
    "plt.plot(history.history['val_loss'], color='orange', label='val_loss')\n",
    "fig_loss.suptitle('Loss', fontsize=20)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc84ce38-852c-452c-99b8-11cabadfe3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig_accuracy = plt.figure()\n",
    "plt.plot(history.history['accuracy'], color='blue', label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], color='green', label='val_accuracy')\n",
    "fig_accuracy.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72866b8-009e-4b23-a335-f5264271664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ec7939-e137-491e-83c6-c854df531f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0193580-f1d3-4317-a15c-8b39f530a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_d30000_b128_e7.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac579357-84db-4b36-93f3-de9e9bc52e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test) #getting predictions on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd13c8-e2bc-4dfe-b2e1-f07f64bcfe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12729116-9ddd-45d7-a0ba-9054634a9c61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
